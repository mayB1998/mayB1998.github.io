<!DOCTYPE HTML>
<html>
	<head>
		<title>Mayur Bhise::Portfolio Website</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body>

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Mayur Bhise</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="index.html">Home</a></li>
											<li><a href="index.html#projects">Projects</a></li>
											<li><a href="index.html#about">About</a></li>
											<li><a href="index.html#contact">Contact</a></li>
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<article id="main">
						<header>
							<h2>Autonomous Navigation</h2>
							<h4>using NUANCE Autonomous Vehicle</h4>
						</header>
						<section class="wrapper style5">
							<div class="inner">

								<div class="row">
									<div class="6u 12u$(medium)">
										<h3>About the Project</h3>

										<p>I've always been interested in autonomous navigation, localization, and Planning, so I seized the opportunity as our Field Robotics Lab had a Autonomous Car<b><a href="https://fieldroboticslab.ece.northeastern.edu/autonomous-car/" target="blank" > Northeastern Robotics</a></b>. This also proved a great opportunity to experiment with Visual Odometry, localization, feature Detectors, pointcloud processing, something that has fascinated me since the beginning of its use in autonomous vehicles.</p>

										<h4>Hardware used:</h4>
										<ul>
											<li><b><a href="https://fieldroboticslab.ece.northeastern.edu/autonomous-car/" target="blank" > NUANCE Autonomous Car</a></b></li>
											<li><b><a href="http://velodynelidar.com/vlp-16.html" target="blank" > Velodyne VLP-16 LIDAR</a></b></li>
											<li><b><a href="https://www.flir.com/products/blackfly-gige/?vertical=machine+vision&segment=iis" target="blank" > 5 pointgray BlackflyS 5MP cameras</a></b></li>
										</ul>
									</div>

									<div class="6u$ 12u$(medium)">
											<span class="image fit"><img src="images/autonomousCar.jpg" alt="NUANCE AUTONOMOUS CAR" /></span>
									</div>

								</div>

								<hr />

								<h3 align="left">Skills Involved</h3>
								<ul>
									<li align="left">ROS Navigation Stack, ROS plugins</li>
									<li align="left">C++ &amp; Python for real-time embedded control</li>
									<li align="left">Motion planning &amp; Visual Odometry</li>
									<!-- <li align="left">Control algorithms</li> -->
									<li align="left">Laser/LIDAR data handling</li>
									<li align="left">Machine perception</li>
									<li align="left"><b><a href="http://pointclouds.org/" target="blank" > PCL (Point Cloud Library)</a></b></li>
									<li align="left">Pointcloud filtering</li>
									<li align="left"><b><a href="http://gazebosim.org/" target="blank" > Gazebo</a></b>, <b><a href="http://wiki.ros.org/rviz" target="blank" > RViz</a></b></li>
									<li align="left">Wireless network issue debugging</li>
									<li align="left">Troubleshooting real robot hardware</li>
								</ul>

								<hr />

								<div class="inner">

									<h3>Procedure</h3>

									<h4>First Steps</h4>
									<p>The initial system setup for any project was to setup the car for collecting data using ROS on linux platform, where I collected the data for the nodes required for performing visual slam, and then I drove the car around an approximate circle in the BOSTON City area to get a loop closure, which are really important to get acccurate maps of the environment without any scale drift.</p>
									<p>The data collected was preprocessed and extracted to get raw images for the Visual slam case, then I created a visual odometry pipeline in python using various feature descriptors such as ORB, FAST, SIFT and compared the performance for all of htem, first with the publicly available KITTI dataset and then with the real data collected</p>
									<p>The pipeline performed well with the KITTI Dataset, because it was collected in ideal conditions without any other moving objects, but when I ran it on the dataset I collected it broke several times areas where the vehicle stopped at traffic stops and other cars were moving at relatively different speeds comapred to us, so I had to implement some heuristics to get it perform well, so I used Gaussian Filtering, image cropping whent he vehicle is stationary to avoid the moving vehicles, which gave satisfactory results <span class="image right"><img src="images/vslam.jpg" alt="navigating an obstacle" /></span></p>

									<h4>SLAM &amp; Autonomous Navigation</h4>
									<p><span class="image right"><img src="images/cropping.jpg" alt="Heuristics &amp; Gaussian Filtering" /></span>The images had to be filtered and cropped a various points where the vehicle was stationary, the road had to be cropped for feature detetion so that it cannot take any erranoues movements into account, the video here shows the implementation after applying the heuristics</p>
									<p><span class="image left"><img src="images/videoplayback.mp4" alt="localization" /></span></p>
									<ol>
										<li>Eliminate any points within a cylinder of radius of 0.2 m, centered on the robot (oriented vertically)</li>
										<li>Crop any points beyond 4 meters in the x and y directions</li>
										<li>Ignore points taller than the robot (0.4 m) or within 1 cm of the ground</li>
										<li>Remove all points below a conic surface angled 1.5&deg; from the ground, centered on the robot and starting at ground height</li>
									</ol>
									<p>The final bullet provides the majority of the benefit. Unlike in simulation, the real robot has manufacturing and assembly tolerances affecting the pose of the laser, and drives on ground that is not completely flat. The 1.5&deg; conical filter eliminates any false detections from the floor due to tilt.</p>
									<p><span class="image right"><img src="images/autonomy3.jpg" alt="navigating an obstacle" /></span>After this, pure odometric collision avoidance was working reliably, and it was time to start building maps! <b><a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping" target="blank" > SLAM</a></b> has made huge strives in functionality and efficiency over the past few decades and is easily implemented using widely-available ROS packages. For this project, the popular <b><a href="http://wiki.ros.org/gmapping" target="blank" > gmapping</a></b> package was used to construct global costmaps, and <b><a href="http://wiki.ros.org/amcl" target="blank" > AMCL</a></b> used to localize within these maps once built.</p>
									<p>The accompanying picture shows Jackal autonomously navigating around the Robotics Lab and visually summarizes many important aspects of the project:</p>
									<ol>
										<li>Yellow represents known obstacles in the global costmap</li>
										<li>Cyan represents the <b><a href="http://wiki.ros.org/costmap_2d/layered#Inflation_Layer" target="blank" > inflation layer</a></b></li>
										<li>Light-gray arrows (mostly clustered beneath the robot) are individual particles of the <b><a href="https://en.wikipedia.org/wiki/Particle_filter" target="blank" > particle filter</a></b></li>
										<li>The <i>thick</i> pink line is the path to the goal generated by the global planner</li>
										<li>The <i>thin</i> pink line represents the local path (what the robot's controller actually commands)</li>
										<li>Scattered red points are individual returns from the LIDAR.</li>
									</ol>

									<h4>Custom Controller &amp; Beyond</h4>
									<p><span class="image right"><img src="images/autonomy4.jpg" alt="clipping corners" /></span>Navigation worked surprisingly well for a single-quarter project, but there were many aspects that could have been improved. A known issue with the current software is the tendency for the controller to cut corners constructed by the global planner. While most of the time this doesn't cause problems, occasionally it leads the robot to clip the very obstacle being circumnavigated. The adjacent picture shows an exaggerated example - the controller is about to drive Jackal into a wall during a simulation run.</p>
									<p>The shortcomings of the <b><a href="http://wiki.ros.org/dwa_local_planner" target="blank" > default controller</a></b> have been known for some time, and other options are readily available as plugins. However, I thought it would be worthwhile (and fun!) to implement a purpose-built controller, one much more ideal for this specific robot and configuration. While this idea came too late to fully incorporate into the final results of the project, it will continue to be worked on throughout intermissions and subsequent quarters at Northwestern.</p>

								</div>

								<hr />

								<h3>Learn More</h3>

								<p> The software is freely available as a ROS package at <b><a href="https://github.com/mayb998/SLAM" target="blank" > https://github.com/mayb998/SLAM/blob/master/README.md" target="blank" > README</a></b> for detailed information for installation and setup instructions for your robot (modifications may be necessary  if using anything other than a Jackal and Velodyne VLP-16). Many open-source packages were used under the hood, so take a look at the package dependencies if you're interested only in a specific portion of the project.</p>

							</div>
						</section>
					</article>


				<!-- Footer -->
					<footer id="footer">
						<ul class="icons">
							<li><a href="https://github.com/mayb1998/" target="blank" class="icon fa-github fa-lg"><span class="label">Github</span></a></li>
							<li><a href="https://www.linkedin.com/in/mayur-bhise" target="blank" class="icon fa-linkedin fa-lg"><span class="label">Email</span></a></li>
							<li><a href="mailto:bhise.m@northeastern.edu" class="icon fa-envelope-o fa-lg"><span class="label">Email</span></a></li>
						</ul>
						<ul class="copyright">
							<li>Template: <a href="https://html5up.net/spectral"> SPECTRAL</a></li><li>From: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
