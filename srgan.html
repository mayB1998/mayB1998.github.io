<!DOCTYPE HTML>
<html>
	<head>
		<title>Mayur Bhise::Portfolio Website</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<link rel="icon" href="images/favicon.ico">
	</head>
	<body>

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Mayur Bhise</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="index.html">Home</a></li>
											<li><a href="index.html#projects">Projects</a></li>
											<li><a href="index.html#about">About</a></li>
											<li><a href="index.html#contact">Contact</a></li>
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<article id="main">
						<header>
							<h2>General Adverserial Networks</h2>
							<h4>Super Resolution GAN</h4>
						</header>
						<section class="wrapper style5">
							<div class="inner">

								<div class="row">

									<div class="6u 12u$(medium)">
										<h3>About the Project</h3>
                    <p>Shifting and Image from High to Low Resolution istermed as Super resolution. Earlier, it was done usingbicubic-interpolation, CNNs, and then GANs wereimplemented; which caused an increase in the quality of theimage produced. GANs consist of two models; a generatorwhich produces images from random noise when trained;and a discriminator which determines whether the generatedimage is fake or not. The task of upscaling lies with theCNN RESnet. The objective is achieved when there is a 0.5probability for discriminating between the generated and theoriginal image. The GAN is a type supervised learningalgorithm which is trained to be able to successfullygenerate unseen upscaled images. In our SRGAN model, weused a VGG19 network to assist in the discriminator's lossscore. The VGG19 is a network developed by a group atOXFORD where they pretrained an image classificationnetwork on the ImageNet dataset. Using the VGG19network helps to decrease the time it takes to train thediscriminator network as it has help from a pre-trainedmodel.</p>
										<p>The use of computer vision, and CNNs has skyrocketed over the past few decades, especially for image classification. For this project, I demonstrated the use of CNNs and GAN for upscaling images from low resolution to High resolution, the upscaling can be used in many applications such as for upscaling old footage, medical footage, satellite imaging<b><a href="http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_shi_tomasi/py_shi_tomasi.html" target="blank" > Shi-Tomasi improved corner detection algorithm</a></b> was coupled with the <b><a href="https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method" target="blank" > Lucas-Kanade optical flow algorithm</a></b>, and the output was then combined with data from an <b><a href="https://en.wikipedia.org/wiki/Inertial_measurement_unit" target="blank" > Inertial Measurement Unit</a></b> (IMU) via an <b><a href="https://en.wikipedia.org/wiki/Extended_Kalman_filter" target="blank" > Extended Kalman Filter</a></b> (EKF) to estimate robot motion.</p>
									</div>

									<div class="6u$ 12u$(medium)">
										<span class="image fit"><img src="images/srgan.jpg" alt="SRGAN architecture" /></span>
									</div>

								</div>

								<hr />

								<!-- <div class="inner"> -->
									<h3 align="left">Skills Involved</h3>
									<ul>
										<li align="left">OpenCV</li>
										<li align="left">Computer Vision</li>
										<li align="left">GAN</li>
										<li align="left">keras and pytorch</li>
										<li align="left">Image processing</li>
										<li align="left">Neural Networks</li>
										<li align="left">Camera stabilization techniques</li>
										<!-- <li align="left">C++</li> -->
									</ul>
								<!-- </div> -->

								<hr />

								<h3 align="left">Procedure</h3>

								<!-- <div class="inner"> -->
									<p>Two different computer vision methods were tried and their results compared. Both methods first estimate the camera motion from optical flow data and use the rigid transform between camera and robot chassis to then estimate robot motion. In method 1, the <b><a href="https://en.wikipedia.org/wiki/Essential_matrix#Use" target="blank"> Essential Matrix</a></b> was decomposed to a transformation matrix (which consists of a rotation and translation). For method 2, a simpler <b><a href="https://en.wikipedia.org/wiki/3D_projection" target="blank"> projective transform</a></b> was used, since the transform between camera and ground frames was known. A projective transform is computationally cheap, requiring only a single matrix multiplication operation for each tracked point.</p>
									<p>The first technique, unfortunately, did not work very well. The Essential Matrix calculation often returned invalid results, and the decomposition process produces 4 unique solutions, so even valid results had to be sifted through to find the correct one. There are many reasons for the invalid returns, including:</p>
									<ul>
										<li align="left">Too little camera motion from frame-to-frame</li>
										<li align="left">Noisy image data (especially when moving)</li>
										<li align="left">Singular planar surfaces (such as floors) are a degenerate case</li>
										<li align="left">Scale ambiguity of the method</li>
									</ul>
								<!-- </div> -->

								<div class="row">
									<div class="6u 12u$(medium)">
										<p>Needless to say, this method was not ideal. Furthermore, it's much more computationally expensive than the 2nd method. Meanwhile, the projective transform worked surprisingly well. It had somewhat large error at high speeds, but that could be overlooked for the purposes of this project.</p>
										<p>After collecting test data and comparing the 2 methods, it was clear that the projective transform was the best method both from an efficiency and accuracy standpoint. The resulting <b><a href="https://en.wikipedia.org/wiki/Visual_odometry" target="blank"> visual odometry</a></b> was also far more accurate than inertial navigation alone (i.e. using only the IMU to estimate state).</p>
										<p>The results of both are shown in the adjacent plots, as well as data for wheel encoder odometry. In the <i>Stationary test</i>, both visual and encoder odometry accumulate almost zero positional change over 35 seconds (as expected). Inertial navigation, however, drifts almost 75 cm and back in that same time frame. A similar result is obtained for the <i>Normal Driving</i> test - the visual and encoder odometry graphs give relatively consistent predictions, while the IMU graph dwarfs them both in scale (note there are 2 separate axes - one for encoder &amp; visual, the other for inertial - which vary by a factor of nearly 50).</p>
									</div>

									<div class="6u$ 12u$(medium)">
										<span class="image fit"><img src="images/compviz2.png" alt="Results of Stationary tests" /></span>
										<span class="image fit"><img src="images/compviz3.png" alt="Results of Normal_Driving tests" /></span>
									</div>
								</div>


								<hr />

								<h3>Learn More</h3>

								<p>This was my final project for <b><a href="https://wl11gp.neu.edu/udcprod8/bwckctlg.p_disp_course_detail?cat_term_in=201630&subj_code_in=EECE&crse_numb_in=5644" target="blank" > EECE 5644: Pattern Recognition and Machine Learning</a></b> at Northeastern. The original paper for the SRGAN implementation is <b><a href="https://arxiv.org/abs/1609.04802" target="blank"</a></b> The <b><a href="https://github.com/leftthomas/SRGAN/blob/master/README.md" target="blank" > README</a></b> should contain all necessary information for SRGAN in pytorch by th author</p>

							</div>
						</section>
					</article>


				<!-- Footer -->
					<footer id="footer">
						<ul class="icons">
							<li><a href="https://github.com/mayb1998/" target="blank" class="icon fa-github fa-lg"><span class="label">Github</span></a></li>
							<li><a href="https://www.linkedin.com/in/mayur-bhise" target="blank" class="icon fa-linkedin fa-lg"><span class="label">Email</span></a></li>
							<li><a href="mailto:bhise.m@northeastern.edu" class="icon fa-envelope-o fa-lg"><span class="label">Email</span></a></li>
						</ul>
						<ul class="copyright">
							<li>Template: <a href="https://html5up.net/spectral"> SPECTRAL</a></li><li>From: <a href="http://html5up.net">HTML5 UP</a></li>						</ul>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
